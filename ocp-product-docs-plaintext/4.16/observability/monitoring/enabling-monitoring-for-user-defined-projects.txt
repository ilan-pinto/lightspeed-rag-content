# Enabling monitoring for user-defined projects


In Red Hat OpenShift Container Platform 4.16, you can enable monitoring for user-defined projects in addition to the default platform monitoring. You can monitor your own projects in Red Hat OpenShift Container Platform without the need for an additional monitoring solution. Using this feature centralizes monitoring for core platform components and user-defined projects.

[NOTE]
----
Versions of Prometheus Operator installed using Operator Lifecycle Manager (OLM) are not compatible with user-defined monitoring. Therefore, custom Prometheus instances installed as a Prometheus custom resource (CR) managed by the OLM Prometheus Operator are not supported in Red Hat OpenShift Container Platform.
----

# Enabling monitoring for user-defined projects

Cluster administrators can enable monitoring for user-defined projects by setting the enableUserWorkload: true field in the cluster monitoring ConfigMap object.


[IMPORTANT]
----
In Red Hat OpenShift Container Platform 4.16 you must remove any custom Prometheus instances before enabling monitoring for user-defined projects.
----


[NOTE]
----
You must have access to the cluster as a user with the cluster-admin cluster role to enable monitoring for user-defined projects in Red Hat OpenShift Container Platform. Cluster administrators can then optionally grant users permission to configure the components that are responsible for monitoring user-defined projects.
----

* You have access to the cluster as a user with the cluster-admin cluster role.
* You have installed the OpenShift CLI (oc).
* You have created the cluster-monitoring-config ConfigMap object.
* You have optionally created and configured the user-workload-monitoring-config ConfigMap object in the openshift-user-workload-monitoring project. You can add configuration options to this ConfigMap object for the components that monitor user-defined projects.

[NOTE]
----
Every time you save configuration changes to the user-workload-monitoring-config ConfigMap object, the pods in the openshift-user-workload-monitoring project are redeployed. It might sometimes take a while for these components to redeploy.
----

1. Edit the cluster-monitoring-config ConfigMap object:

```terminal
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
```

2. Add enableUserWorkload: true under data/config.yaml:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true 1
```

When set to true, the enableUserWorkload parameter enables monitoring for user-defined projects in a cluster.
3. Save the file to apply the changes. Monitoring for user-defined projects is then enabled automatically.

[NOTE]
----
If you enable monitoring for user-defined projects, the user-workload-monitoring-config ConfigMap object is created by default.
----

[WARNING]
----
When changes are saved to the cluster-monitoring-config ConfigMap object, the pods and other resources in the openshift-monitoring project might be redeployed. The running monitoring processes in that project might also be restarted.
----
4. Verify that the prometheus-operator, prometheus-user-workload, and thanos-ruler-user-workload pods are running in the openshift-user-workload-monitoring project. It might take a short while for the pods to start:

```terminal
$ oc -n openshift-user-workload-monitoring get pod
```

Example output

```terminal
NAME                                   READY   STATUS        RESTARTS   AGE
prometheus-operator-6f7b748d5b-t7nbg   2/2     Running       0          3h
prometheus-user-workload-0             4/4     Running       1          3h
prometheus-user-workload-1             4/4     Running       1          3h
thanos-ruler-user-workload-0           3/3     Running       0          3h
thanos-ruler-user-workload-1           3/3     Running       0          3h
```


* Creating a user-defined workload monitoring config map
* Configuring the monitoring stack
* Granting users permission to configure monitoring for user-defined projects

# Granting users permission to monitor user-defined projects

As a cluster administrator, you can monitor all core Red Hat OpenShift Container Platform and user-defined projects.

You can also grant developers and other users different permissions:

* To monitor user-defined projects.
* To configure the components that monitor user-defined projects.
* To configure alert routing for user-defined projects.

You can grant the permissions by assigning one of the following monitoring roles:



The following sections provide details on how to assign these roles by using the Red Hat OpenShift Container Platform web console or the CLI.

## Granting user permissions by using the web console

You can grant users permissions to monitor their own projects, by using the Red Hat OpenShift Container Platform web console.

* You have access to the cluster as a user with the cluster-admin cluster role.
* The user account that you are assigning the role to already exists.

1. In the Administrator perspective within the Red Hat OpenShift Container Platform web console, navigate to User Management -> RoleBindings -> Create binding.
2. In the Binding Type section, select the "Namespace Role Binding" type.
3. In the Name field, enter a name for the role binding.
4. In the Namespace field, select the user-defined project where you want to grant the access.

[IMPORTANT]
----
The monitoring role will be bound to the project that you apply in the Namespace field. The permissions that you grant to a user by using this procedure will apply only to the selected project.
----
5. Select monitoring-rules-view, monitoring-rules-edit, or monitoring-edit in the Role Name list.
6. In the Subject section, select User.
7. In the Subject Name field, enter the name of the user.
8. Select Create to apply the role binding.

## Granting user permissions by using the CLI

You can grant users permissions to monitor their own projects, by using the OpenShift CLI (oc).

* You have access to the cluster as a user with the cluster-admin cluster role.
* The user account that you are assigning the role to already exists.
* You have installed the OpenShift CLI (oc).

* Assign a monitoring role to a user for a project:

```terminal
$ oc policy add-role-to-user <role> <user> -n <namespace> 1
```

Substitute <role> with monitoring-rules-view, monitoring-rules-edit, or monitoring-edit.

[IMPORTANT]
----
Whichever role you choose, you must bind it against a specific project as a cluster administrator.
----

As an example, substitute <role> with monitoring-edit, <user> with johnsmith, and <namespace> with ns1. This assigns the user johnsmith permission to set up metrics collection and to create alerting rules in the ns1 namespace.

# Granting users permission to configure monitoring for user-defined projects

As a cluster administrator, you can assign the user-workload-monitoring-config-edit role to a user. This grants permission to configure and manage monitoring for user-defined projects without giving them permission to configure and manage core Red Hat OpenShift Container Platform monitoring components.

* You have access to the cluster as a user with the cluster-admin cluster role.
* The user account that you are assigning the role to already exists.
* You have installed the OpenShift CLI (oc).

1. Assign the user-workload-monitoring-config-edit role to a user in the openshift-user-workload-monitoring project:

```terminal
$ oc -n openshift-user-workload-monitoring adm policy add-role-to-user \
  user-workload-monitoring-config-edit <user> \
  --role-namespace openshift-user-workload-monitoring
```

2. Verify that the user is correctly assigned to the user-workload-monitoring-config-edit role by displaying the related role binding:

```terminal
$ oc describe rolebinding <role_binding_name> -n openshift-user-workload-monitoring
```

Example command

```terminal
$ oc describe rolebinding user-workload-monitoring-config-edit -n openshift-user-workload-monitoring
```

Example output

```terminal
Name:         user-workload-monitoring-config-edit
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  Role
  Name:  user-workload-monitoring-config-edit
Subjects:
  Kind  Name  Namespace
  ----  ----  ---------
  User  user1           1
```

In this example, user1 is assigned to the user-workload-monitoring-config-edit role.

# Accessing metrics from outside the cluster for custom applications

You can query Prometheus metrics from outside the cluster when monitoring your own services with user-defined projects. Access this data from outside the cluster by using the thanos-querier route.

This access only supports using a Bearer Token for authentication.

* You have deployed your own service, following the "Enabling monitoring for user-defined projects" procedure.
* You are logged in to an account with the cluster-monitoring-view cluster role, which provides permission to access the Thanos Querier API.
* You are logged in to an account that has permission to get the Thanos Querier API route.

[NOTE]
----
If your account does not have permission to get the Thanos Querier API route, a cluster administrator can provide the URL for the route.
----

1. Extract an authentication token to connect to Prometheus by running the following command:

```terminal
$ TOKEN=$(oc whoami -t)
```

2. Extract the thanos-querier API route URL by running the following command:

```terminal
$ HOST=$(oc -n openshift-monitoring get route thanos-querier -ojsonpath={.spec.host})
```

3. Set the namespace to the namespace in which your service is running by using the following command:

```terminal
$ NAMESPACE=ns1
```

4. Query the metrics of your own services in the command line by running the following command:

```terminal
$ curl -H "Authorization: Bearer $TOKEN" -k "https://$HOST/api/v1/query?" --data-urlencode "query=up{namespace='$NAMESPACE'}"
```


The output shows the status for each application pod that Prometheus is scraping:
Example output

```terminal
{"status":"success","data":{"resultType":"vector","result":[{"metric":{"__name__":"up","endpoint":"web","instance":"10.129.0.46:8080","job":"prometheus-example-app","namespace":"ns1","pod":"prometheus-example-app-68d47c4fb6-jztp2","service":"prometheus-example-app"},"value":[1591881154.748,"1"]}]}}
```


* Enabling monitoring for user-defined projects

# Excluding a user-defined project from monitoring

Individual user-defined projects can be excluded from user workload monitoring. To do so, add the openshift.io/user-monitoring label to the project&#8217;s namespace with a value of false.

1. Add the label to the project namespace:

```terminal
$ oc label namespace my-project 'openshift.io/user-monitoring=false'
```

2. To re-enable monitoring, remove the label from the namespace:

```terminal
$ oc label namespace my-project 'openshift.io/user-monitoring-'
```


[NOTE]
----
If there were any active monitoring targets for the project, it may take a few minutes for Prometheus to stop scraping them after adding the label.
----

# Disabling monitoring for user-defined projects

After enabling monitoring for user-defined projects, you can disable it again by setting enableUserWorkload: false in the cluster monitoring ConfigMap object.


[NOTE]
----
Alternatively, you can remove enableUserWorkload: true to disable monitoring for user-defined projects.
----

1. Edit the cluster-monitoring-config ConfigMap object:

```terminal
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
```

1. Set enableUserWorkload: to false under data/config.yaml:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: false
```

2. Save the file to apply the changes. Monitoring for user-defined projects is then disabled automatically.
3. Check that the prometheus-operator, prometheus-user-workload and thanos-ruler-user-workload pods are terminated in the openshift-user-workload-monitoring project. This might take a short while:

```terminal
$ oc -n openshift-user-workload-monitoring get pod
```

Example output

```terminal
No resources found in openshift-user-workload-monitoring project.
```



[NOTE]
----
The user-workload-monitoring-config ConfigMap object in the openshift-user-workload-monitoring project is not automatically deleted when monitoring for user-defined projects is disabled. This is to preserve any custom configurations that you may have created in the ConfigMap object.
----

# Next steps

* Managing metrics